= Updating AWS Classic cluster
== Overview

The aws classic clusters have to be upgraded to keep cluster compliant.

Below process will help you to upgrade a AWS Classic Cluster Safely

== Prerequisites

. You have a ticket assigned to upgrade a cluster node pool version
. CCM has been created and approved for the changes 
. You have access to AWS cluster from Console ( eg dmc-prod , dmc-staging , dmc-dev)
. You have Cluster access from terminal eg kc1 prod && kubectl get nodes
. Post in #cloud-alerts

== Steps

There are 2 Major steps

. Engine Upgrade

. Node Upgrade

== Step 1 - Engine Upgrade
[,shell]
----
kc1 <env> aws <cluster_uuid> 
kubectl get nodes -owide -w
----

IMPORTANT NOTE!!!: what happens if the cluster is multiregion(MR) and we need to upgrade all datacenters?
In that case, each dc will have its own cloud_account + region,so we can get that info from Orchestrator DB
Run [get-cluster-metadata.sh](../../common/utils/get-cluster-metadata.sh) to find the cluster metadata for the cluster 

```shell
./get-cluster-metadata.sh -e <env> -c <cluster_uuid_dc-N>
```
So with this info we execute the EKS (Engine) upgrade and can create the new template as explained below

. Once you have the cluster metadata,open AWS SSO chiclet from okta and search for the account
. Go to EKS (Elastic Kubernetes Service) for the cluster that needs to be upgraded (remember to select its region).
. Once in EKS Cluster Engine, click upgrade and upgrade to the targeted version i.e. v1.23 (estimated time is 20-30 mins)
  Wait until the upgrade is complete - once we upgrade a version it will push a notification on top

Note: we cannot upgrade version N to N+2, it has to go from version N to N+1 and then from N+1 to N+2

. Go to Launch Template in EC2
. Check the cluster and pick the template which matches the first few characters of the UUID of the cluster
. Select latest version

. Click on Actions and click on ‘Modify template’
. Search for the AMI by clicking on ‘Browse more AMIs’ - click on community ami and select ** amazon-eks-node-1.23-v20230304 ** (targeted version v1.23)
. Scroll to ‘Storage (volumes)’ section Expand Volume 1 and Change the 'Size (GiB)' to 100 GB from its default value (20) and Change "Encryption" to 'Don't include in launch template'
. Check for the label in Advanced section ** labelFlag='--node-labels=node.kubernetes.io/requested_role=worker' **
. Click on ‘Create template version’
Once above is done we can start the node upgrades on the cluster following below steps 

== Step 2: Node Upgrade

==== Step1: Login to the cluster and list the nodes not on the target version, e.g. if your target version is 1.23:
[,shell]
----
kc1 <env> aws <account>
kubectl get node -o wide | grep -v '1.23'
----
==== Step2: Execute Kubectl cordon nodes for each node:
----
for i in `kubectl get nodes | grep -v NAME | awk '{print $1}'`; do kubectl cordon $i; done
----

==== Step3: go to the relevant cluster ASG and make the desired node count twice of the existing and wait for the new nodes to come Ready
[,shell]
----
kubectl get nodes -o wide -w
----

Once the new nodes are up in the targeted version - we can start a rollout restart of pods.

On a fresh terminal run `kubectl get pods -owide -w` to monitor the pods 

==== Step4: Execute a rolling restart of DSE nodes/pods
[,shell]
----
for pod in $(kubectl  get po | grep caas-cluster-dc | awk '{print $1}'); do kubectl exec -it $pod -c cassandra — nodetool flush; done
----
Note: deamonset ingress and fluent bit will rollout restart with dse pods

rollout restart the statefulset (one by one and one rack at a time)

eg:
[,shell]
----
kubectl rollout restart sts caas-cluster-dc-1-rack0
----
Make sure rack 0 has all pods up and running before moving to next one

==== Step5: Once all the DSE pods are running, do a rolling restart on Stargate pods
[,shell]
----
kubectl rollout restart sts stargate
----
Wait until all Stargete pods are restarted before movig to the next stateful set

. Execute rolling restart on Vector 
[,shell]
----
kubectl rollout restart sts vector
----

==== Step6: Execute rolling restart of other pods in the cluster
Make sure all pods are up and running after every restart
[,shell]
----
kubectl rollout restart deploy cmd-processor fluent-bit webcql
----
[,shell]
----
kubectl rollout restart deploy cass-operator grafana
----
[,shell]
----
kubectl rollout restart deploy opa prometheus
----
[,shell]
----
kubectl rollout restart deploy astra-operator
----

==== Step7: Execute a rolling restart on core-dns, One at a time delete each coredns pod
[,shell]
----
kubectl get po -n kube-system | grep coredns 
----

==== Step8: Delete teleport pod 
[,shell]
----
kubectl get pod -n teleport-agent

kubectl delete pod <pod-name> -n teleport-agent
----



CHECKS:

. Grafana dashboards
. SLA Health Checker Dashboard Monitoring
Keep the SLA Healthchecker Dashboard open and monitor if there are prolonged errors
. Status of all the pods in cluster
Monitor all pods and nodes and wait for all pods to come into running state

Keep monitoring state of Kube from Terminal

[,shell]
----
kubectl get pods -A -o wide | grep -v 'Running'

kubectl get nodes -owide
----


== Step 3: Completion Notification

Mark the Task Completed and update the status and issues faced

Mark the CCM Completed

Update DBPE team in slack channel #pe-collab


=== Troubleshooting
1 DSE Pod in Pending State for ~10 minutes

==== Check for the labels. If they are in ReadytoStart we need to overwrite to Starting

[,shell]
----
k get po --show-labels
k get pod -L cassandra.datastax.com/node-state
k label --overwrite po <pod_name> cassandra.datastax.com/node-state=Starting
exec into that pod and run which will start the C* process

curl --unix-socket /tmp/dse-mgmt.sock -X POST 'http://localhost/api/v0/lifecycle/start'
k label --overwrite po <pod_name> cassandra.datastax.com/node-state=Started
repeat for rest of the cass pods
----

==== Compare the capacity of the new nodes with the cordoned ones. 
Go to the relevant cluster ASG and click on Instance management to check the instance type.
Please make sure the new nodes are of the same instance type as the cordoned nodes.

To correct the instance types, we can launch new AMI template with the required changes

. Go to Launch Template in EC2
. Check the cluster and pick the template which matches the first few characters of the UUID of the cluster
. Select latest version
. Click on Actions and click on ‘Modify template’
. Make sure the AMI is in targeted version
. Scroll to Instance type and choose the right one
. Double check the storage and label in Advanced section
. Click on ‘Create template version’

Now delete the new nodes with capacity restrictions and repeat step3

==== Corrupt sstables
If you see the below errors in the pod logs 

```

INFO  [pool-2-thread-1] 2023-11-01 19:07:17,921 LifecycleResources.java:144 - Started DSE                                                                                                                                                                  │
│ INFO  [nioEventLoopGroup-2-2] 2023-11-01 19:07:23,837 UnixSocketCQLAccess.java:88 - Cannot create internal Driver CQLSession as the driver socket has not been created. This should resolve once DSE has started and created the socket at /tmp/dse.sock   │
│ INFO  [nioEventLoopGroup-2-2] 2023-11-01 19:07:33,831 UnixSocketCQLAccess.java:88 - Cannot create internal Driver CQLSession as the driver socket has not been created. This should resolve once DSE has started and created the socket at /tmp/dse.sock   │

```

check debug.log file by going into the pod /var/log/cassandra and move the corrupt sstable out

eg :

```
mv /var/lib/cassandra/data/eventprocessing/event_audit_log-9ce6cc904dc911edba032b7aa982ac2d/.event_changed_attr_entry_idx/bb-80-bti-Data.db .
```
